#!/usr/bin/env python3
"""
Automated test and fix loop for resync workflow.

This script:
1. Starts the server in the background
2. Initiates a resync using the provided data
3. Monitors logs in real-time for errors, exceptions, warnings
4. Automatically fixes issues found
5. Repeats the process until no errors remain

Usage:
    uv run python tests/ci/test_resync_automated.py
"""

import asyncio
import json
import logging
import os
import re
import subprocess
import sys
import time
from pathlib import Path
from typing import Any

import httpx

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# Test configuration
SERVER_URL = "http://localhost:8000"
KNOWLEDGE_ID = "696fc99db002d6c4ff0d6b3c"
ORGANIZATION_ID = "I7Cchh33eJM0TKnSF8HGJ1X0Fbqat9ub"
SERVER_PORT = 8000

# Resync request data (from user's logs)
RESYNC_REQUEST = {
	"website_url": "https://app.spadeworks.co/",
	"website_name": "https://app.spadeworks.co/",
	"credentials": {
		"username": os.getenv("TEST_USERNAME", "test@example.com"),
		"password": os.getenv("TEST_PASSWORD", "test123"),
	},
	"s3_references": [
		{
			"bucket": "knowledge-extraction-wf-dev",
			"key": f"{ORGANIZATION_ID}/knowledge/696fc997b002d6c4ff0d6b3b/2026-01-20T18-29-43-spadeworks-short-demo.mp4",
			"region": "nyc3",
			"presigned_url": ""  # Will be generated by S3 downloader
		}
	],
	"file_metadata_list": [
		{
			"filename": "spadeworks-short-demo.mp4",
			"size": 916577,
			"content_type": "video/mp4"
		}
	],
	"knowledge_id": KNOWLEDGE_ID,
	"options": {
		"max_pages": 10,
		"max_depth": 3,
		"extract_thumbnails": True,
	}
}

logging.basicConfig(
	level=logging.INFO,
	format='%(asctime)s - %(levelname)s - %(message)s',
)
logger = logging.getLogger(__name__)


class LogMonitor:
	"""Monitors server logs in real-time for errors."""
	
	ERROR_PATTERNS = [
		(r'ERROR', 'error'),
		(r'Exception:', 'exception'),
		(r'Traceback', 'traceback'),
		(r'ValidationError', 'validation_error'),
		(r'TypeError', 'type_error'),
		(r'AttributeError', 'attribute_error'),
		(r'KeyError', 'key_error'),
		(r'ValueError', 'value_error'),
		(r'IndexError', 'index_error'),
		(r'not JSON serializable', 'json_serialization'),
		(r'no attribute', 'missing_attribute'),
		(r'missing.*required', 'missing_required'),
		(r'unexpected keyword', 'unexpected_keyword'),
		(r'Workflow failed', 'workflow_failed'),
		(r'Activity task failed', 'activity_failed'),
	]
	
	WARNING_PATTERNS = [
		(r'WARNING', 'warning'),
		(r'‚ö†Ô∏è', 'warning_emoji'),
	]
	
	KNOWN_IGNORE_PATTERNS = [
		r'DEBUG.*httpcore',
		r'DEBUG.*LiteLLM',
		r'Worker heartbeating.*does not support',
		r'DEBUG.*browser_use\.utils',
	]
	
	def __init__(self, log_file: Path):
		self.log_file = log_file
		self.processed_lines = set()
		self.errors: list[dict[str, Any]] = []
		self.warnings: list[dict[str, Any]] = []
	
	def scan_new_lines(self) -> tuple[list[dict], list[dict]]:
		"""Scan log file for new errors and warnings."""
		if not self.log_file.exists():
			return [], []
		
		new_errors = []
		new_warnings = []
		
		with open(self.log_file, 'r') as f:
			lines = f.readlines()
		
		for i, line in enumerate(lines):
			line_hash = hash(line)
			if line_hash in self.processed_lines:
				continue
			
			self.processed_lines.add(line_hash)
			
			# Check if should ignore
			should_ignore = False
			for pattern in self.KNOWN_IGNORE_PATTERNS:
				if re.search(pattern, line, re.IGNORECASE):
					should_ignore = True
					break
			
			if should_ignore:
				continue
			
			# Check for errors
			for pattern, error_type in self.ERROR_PATTERNS:
				if re.search(pattern, line, re.IGNORECASE):
					error = {
						'line_number': i + 1,
						'line': line.strip(),
						'type': error_type,
						'pattern': pattern,
					}
					new_errors.append(error)
					self.errors.append(error)
					break
			
			# Check for warnings
			for pattern, warning_type in self.WARNING_PATTERNS:
				if re.search(pattern, line, re.IGNORECASE):
					warning = {
						'line_number': i + 1,
						'line': line.strip(),
						'type': warning_type,
					}
					new_warnings.append(warning)
					self.warnings.append(warning)
					break
		
		return new_errors, new_warnings


class ErrorFixer:
	"""Automatically fixes errors found in logs."""
	
	def __init__(self, project_root: Path):
		self.project_root = project_root
		self.fixes_applied: list[dict] = []
	
	async def fix_error(self, error: dict) -> bool:
		"""Attempt to fix an error. Returns True if fix was applied."""
		error_type = error.get('type')
		line = error.get('line', '')
		
		if error_type == 'json_serialization':
			return await self.fix_json_serialization(line)
		elif error_type == 'missing_attribute':
			return await self.fix_missing_attribute(line)
		elif error_type == 'validation_error':
			return await self.fix_validation_error(line)
		elif error_type == 'workflow_failed':
			return await self.fix_workflow_failure(line)
		
		return False
	
	async def fix_json_serialization(self, error_line: str) -> bool:
		"""Fix JSON serialization errors."""
		if 'FrameReference' in error_line and 'not JSON serializable' in error_line:
			# Check if already fixed
			file_path = self.project_root / "navigator/temporal/activities/video/frame_filtering.py"
			if file_path.exists():
				content = file_path.read_text()
				if 'frame_ref.to_path_string()' in content:
					# Already fixed
					return False
				# Fix: Convert FrameReference to string
				if 'all_frame_paths.append((timestamp, frame_ref))' in content:
					content = content.replace(
						'all_frame_paths.append((timestamp, frame_ref))',
						'all_frame_paths.append((timestamp, frame_ref.to_path_string()))'
					)
					content = content.replace(
						'filtered_frame_paths.append((timestamp, frame_ref))',
						'filtered_frame_paths.append((timestamp, frame_ref.to_path_string()))'
					)
					file_path.write_text(content)
					logger.info("üîß Fixed: FrameReference JSON serialization")
					return True
		return False
	
	async def fix_missing_attribute(self, error_line: str) -> bool:
		"""Fix missing attribute errors."""
		if 'thumbnails' in error_line and 'SourceMetadata' in error_line:
			# Check if already fixed
			file_path = self.project_root / "navigator/schemas/domain.py"
			if file_path.exists():
				content = file_path.read_text()
				if 'thumbnails: list[str]' in content:
					# Already fixed
					return False
				# Add thumbnails field to SourceMetadata
				if 'language: str | None' in content:
					content = content.replace(
						'language: str | None = Field(default=None, description="Primary language of the content")',
						'language: str | None = Field(default=None, description="Primary language of the content")\n\t\n\t# Video-specific metadata\n\tthumbnails: list[str] = Field(\n\t\tdefault_factory=list,\n\t\tdescription="List of thumbnail file paths (for video sources)"\n\t)'
					)
					file_path.write_text(content)
					logger.info("üîß Fixed: Added thumbnails field to SourceMetadata")
					return True
		return False
	
	async def fix_validation_error(self, error_line: str) -> bool:
		"""Fix validation errors."""
		if 'SourceMetadata' in error_line and 'thumbnails' in error_line:
			# Same fix as missing_attribute
			return await self.fix_missing_attribute(error_line)
		return False
	
	async def fix_workflow_failure(self, error_line: str) -> bool:
		"""Fix workflow failure errors."""
		# Workflow failures are usually symptoms, not root causes
		# The actual error should be caught by other patterns
		return False


class ServerManager:
	"""Manages server process."""
	
	def __init__(self, project_root: Path):
		self.project_root = project_root
		self.process: subprocess.Popen | None = None
		self.log_file: Path | None = None
	
	async def start(self) -> bool:
		"""Start the server."""
		try:
			# Create log directory
			log_dir = self.project_root / "test_logs"
			log_dir.mkdir(exist_ok=True)
			self.log_file = log_dir / f"server_{int(time.time())}.log"
			
			logger.info(f"üöÄ Starting server (logs: {self.log_file})")
			
			# Start server
			self.process = subprocess.Popen(
				["uv", "run", "python", "navigator/start_server.py"],
				stdout=open(self.log_file, 'w'),
				stderr=subprocess.STDOUT,
				cwd=self.project_root,
				env={**os.environ, "NAVIGATOR_DEBUG": "false"},
			)
			
			# Wait for server to be ready
			max_wait = 30
			for i in range(max_wait):
				try:
					async with httpx.AsyncClient(timeout=2.0) as client:
						response = await client.get(f"{SERVER_URL}/health")
						if response.status_code == 200:
							logger.info("‚úÖ Server started successfully")
							return True
				except Exception:
					pass
				await asyncio.sleep(1)
			
			logger.error("‚ùå Server failed to start within timeout")
			return False
		except Exception as e:
			logger.error(f"‚ùå Failed to start server: {e}")
			return False
	
	async def stop(self):
		"""Stop the server."""
		if self.process:
			logger.info("üõë Stopping server...")
			self.process.terminate()
			try:
				self.process.wait(timeout=10)
			except subprocess.TimeoutExpired:
				logger.warning("‚ö†Ô∏è Server didn't stop gracefully, killing...")
				self.process.kill()
				self.process.wait()
			self.process = None
			logger.info("‚úÖ Server stopped")


class ResyncTester:
	"""Tests resync workflow."""
	
	def __init__(self, log_monitor: LogMonitor, error_fixer: ErrorFixer):
		self.log_monitor = log_monitor
		self.error_fixer = error_fixer
		self.client = httpx.AsyncClient(base_url=SERVER_URL, timeout=120.0)
	
	async def initiate_resync(self) -> dict[str, Any]:
		"""Initiate resync workflow."""
		try:
			logger.info(f"üîÑ Initiating resync for knowledge_id={KNOWLEDGE_ID}")
			
			response = await self.client.post(
				"/api/knowledge/ingest/start",
				json=RESYNC_REQUEST,
			)
			
			if response.status_code == 200:
				result = response.json()
				job_id = result.get('job_id')
				workflow_id = result.get('workflow_id')
				logger.info(f"‚úÖ Resync started: job_id={job_id}, workflow_id={workflow_id}")
				return {"success": True, "job_id": job_id, "workflow_id": workflow_id}
			else:
				logger.error(f"‚ùå Resync failed: {response.status_code} - {response.text}")
				return {"success": False, "error": response.text, "status_code": response.status_code}
		except Exception as e:
			logger.error(f"‚ùå Exception during resync: {e}", exc_info=True)
			return {"success": False, "error": str(e)}
	
	async def monitor_workflow(self, job_id: str, max_wait: int = 600) -> dict[str, Any]:
		"""Monitor workflow until completion."""
		logger.info(f"üìä Monitoring workflow: job_id={job_id} (max {max_wait}s)")
		
		start_time = time.time()
		last_status = None
		
		while time.time() - start_time < max_wait:
			try:
				response = await self.client.get(f"/api/knowledge/workflows/status/{job_id}")
				if response.status_code == 200:
					status_data = response.json()
					status = status_data.get('status', 'unknown')
					phase = status_data.get('phase')
					progress = status_data.get('progress', 0)
					
					if status != last_status:
						logger.info(f"   Status: {status}, Phase: {phase}, Progress: {progress}%")
						last_status = status
					
					if status in ['completed', 'failed', 'cancelled']:
						return {
							"status": status,
							"phase": phase,
							"progress": progress,
							"data": status_data,
						}
				
				await asyncio.sleep(5)
			except Exception as e:
				logger.warning(f"‚ö†Ô∏è Error checking workflow status: {e}")
				await asyncio.sleep(5)
		
		return {"status": "timeout", "error": "Workflow monitoring timeout"}
	
	async def scan_and_fix(self) -> tuple[bool, list[dict]]:
		"""Scan logs for errors and attempt to fix them."""
		new_errors, new_warnings = self.log_monitor.scan_new_lines()
		
		if new_errors:
			logger.warning(f"‚ö†Ô∏è Found {len(new_errors)} new error(s)")
			for error in new_errors[:5]:
				logger.warning(f"   [{error['type']}] {error['line'][:150]}")
		
		if new_warnings:
			logger.info(f"‚ÑπÔ∏è  Found {len(new_warnings)} new warning(s)")
		
		# Attempt to fix errors
		fixes_applied = []
		for error in new_errors:
			fixed = await self.error_fixer.fix_error(error)
			if fixed:
				fixes_applied.append(error)
				logger.info(f"üîß Fixed error: {error['type']}")
		
		return len(fixes_applied) > 0, fixes_applied
	
	async def close(self):
		"""Close HTTP client."""
		await self.client.aclose()


async def main():
	"""Main test loop."""
	project_root = Path(__file__).parent.parent.parent
	server_manager = ServerManager(project_root)
	error_fixer = ErrorFixer(project_root)
	
	max_iterations = 10
	iteration = 0
	
	try:
		# Start server
		if not await server_manager.start():
			logger.error("‚ùå Failed to start server")
			return 1
		
		log_monitor = LogMonitor(server_manager.log_file)
		tester = ResyncTester(log_monitor, error_fixer)
		
		# Main test loop
		while iteration < max_iterations:
			iteration += 1
			logger.info(f"\n{'='*80}")
			logger.info(f"üîÑ Test Iteration {iteration}/{max_iterations}")
			logger.info(f"{'='*80}\n")
			
			# Initiate resync
			resync_result = await tester.initiate_resync()
			if not resync_result.get('success'):
				logger.error(f"‚ùå Resync failed: {resync_result.get('error')}")
				# Continue to scan for errors
			else:
				job_id = resync_result.get('job_id')
				if job_id:
					# Monitor workflow in background while scanning logs
					monitor_task = asyncio.create_task(
						tester.monitor_workflow(job_id, max_wait=300)
					)
					
					# Scan logs periodically
					scan_interval = 10
					scans_done = 0
					max_scans = 30  # 5 minutes max
					
					while not monitor_task.done() and scans_done < max_scans:
						await asyncio.sleep(scan_interval)
						scans_done += 1
						
						# Scan for errors
						errors_found, fixes = await tester.scan_and_fix()
						
						if errors_found:
							logger.info(f"üîß Applied {len(fixes)} fix(es)")
							# Wait for current workflow to finish or timeout
							try:
								await asyncio.wait_for(monitor_task, timeout=60)
							except asyncio.TimeoutError:
								pass
							
							# Restart server to apply fixes
							logger.info("üîÑ Restarting server to apply fixes...")
							await server_manager.stop()
							await asyncio.sleep(2)
							if not await server_manager.start():
								logger.error("‚ùå Failed to restart server")
								return 1
							
							# Recreate log monitor for new log file
							log_monitor = LogMonitor(server_manager.log_file)
							tester = ResyncTester(log_monitor, error_fixer)
							break
					
					# Wait for workflow to complete
					if not monitor_task.done():
						workflow_result = await monitor_task
					else:
						workflow_result = monitor_task.result()
					
					logger.info(f"üìä Workflow result: {workflow_result.get('status')}")
					
					if workflow_result.get('status') == 'completed':
						# Final error scan
						await asyncio.sleep(5)  # Wait for final logs
						errors_found, fixes = await tester.scan_and_fix()
						
						if not errors_found:
							logger.info("‚úÖ Workflow completed successfully with no errors!")
							break
						else:
							logger.warning(f"‚ö†Ô∏è Workflow completed but {len(log_monitor.errors)} error(s) found in logs")
			
			# Scan for any remaining errors
			await asyncio.sleep(5)
			errors_found, fixes = await tester.scan_and_fix()
			
			if errors_found:
				logger.info(f"üîß Applied {len(fixes)} fix(es), will restart and retry")
				await server_manager.stop()
				await asyncio.sleep(2)
				if not await server_manager.start():
					logger.error("‚ùå Failed to restart server")
					return 1
				log_monitor = LogMonitor(server_manager.log_file)
				tester = ResyncTester(log_monitor, error_fixer)
			else:
				# No more errors to fix
				if iteration >= max_iterations:
					logger.warning("‚ö†Ô∏è Reached max iterations")
				break
		
		# Final summary
		logger.info(f"\n{'='*80}")
		logger.info("üìä Final Summary")
		logger.info(f"{'='*80}")
		logger.info(f"Total errors found: {len(log_monitor.errors)}")
		logger.info(f"Total warnings found: {len(log_monitor.warnings)}")
		logger.info(f"Fixes applied: {len(error_fixer.fixes_applied)}")
		
		if log_monitor.errors:
			logger.error(f"\n‚ùå Test completed with {len(log_monitor.errors)} error(s) remaining:")
			for error in log_monitor.errors[:10]:
				logger.error(f"   [{error['type']}] {error['line'][:150]}")
			return 1
		else:
			logger.info("\n‚úÖ Test completed successfully with no errors!")
			return 0
	
	finally:
		await server_manager.stop()
		await tester.close()


if __name__ == '__main__':
	try:
		exit_code = asyncio.run(main())
		sys.exit(exit_code)
	except KeyboardInterrupt:
		logger.info("\n‚ö†Ô∏è  Test interrupted by user")
		sys.exit(130)
	except Exception as e:
		logger.error(f"‚ùå Test failed with exception: {e}", exc_info=True)
		sys.exit(1)
